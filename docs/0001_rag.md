# RAG Endpoint Implementation Task

## Overview
Create a FastAPI endpoint that implements Retrieval-Augmented Generation (RAG) using Pinecone for vector search and OpenAI GPT-4o-mini for response generation. The endpoint should return structured responses with source references similar to Perplexity.

## Requirements

### 1. Endpoint Specification
- **Route**: `POST /api/rag/query`
- **Input**: JSON payload with user question
- **Output**: Structured JSON with AI response and source references

### 2. Request Schema
```python
class RAGQueryRequest(BaseModel):
    question: str = Field(..., min_length=1, max_length=1000, description="User's question")
    max_results: int = Field(default=5, ge=1, le=10, description="Maximum number of search results to retrieve")
    temperature: float = Field(default=0.7, ge=0.0, le=1.0, description="OpenAI temperature parameter")
```

### 3. Response Schema
```python
class SourceReference(BaseModel):
    id: str = Field(..., description="Document ID from Pinecone")
    title: str = Field(..., description="Document title")
    url: str = Field(..., description="Source URL")
    chunk_info: str = Field(..., description="Chunk information (e.g., 'Chunk: #8.0')")
    relevance_score: float = Field(..., description="Similarity score from Pinecone")
    audience: str = Field(..., description="Target audience (e.g., 'pet-owner')")
    authority_level: str = Field(..., description="Authority level (e.g., 'expert')")
    publisher: str = Field(..., description="Publisher name")
    publication_year: int = Field(..., description="Publication year")

class RAGResponse(BaseModel):
    answer: str = Field(..., description="AI-generated response to the user's question")
    sources: List[SourceReference] = Field(..., description="List of source documents used")
    query_metadata: dict = Field(..., description="Query processing metadata")
```

### 4. Implementation Steps

#### Step 1: Set up Dependencies
```python
# Add these to your requirements.txt or install directly:
# pinecone-client
# openai>=1.0.0
# fastapi
# pydantic

from pinecone import Pinecone
from openai import AsyncOpenAI
import json
import logging
from typing import List, Dict, Any
```

#### Step 2: Configuration
- Add Pinecone API key, index name, and environment to your settings
- Add OpenAI API key to settings
- Configure appropriate timeout and retry settings

#### Step 3: Core RAG Function
Create an async function that:
1. **Queries Pinecone**: Takes user question, converts to embeddings if needed, searches vector database
2. **Processes Results**: Extracts and structures the returned metadata and content
3. **Generates Response**: Sends context + question to OpenAI GPT-4o-mini
4. **Formats Output**: Returns structured response with references

#### Step 4: OpenAI Prompt Engineering
Design a system prompt that:
- Instructs the model to act as an expert in the domain (appears to be veterinary/pet care)
- Requires citing sources using numbered references [1], [2], etc.
- Emphasizes accuracy and instructs to only use provided context
- Handles cases where context doesn't contain sufficient information

Example prompt structure:
```
You are an expert veterinary assistant. Answer the user's question based ONLY on the provided context documents. 

Context Documents:
[Document 1] Title: {title} - {content}
[Document 2] Title: {title} - {content}
...

User Question: {user_question}

Instructions:
- Provide a comprehensive, accurate answer
- Use numbered references [1], [2] to cite specific documents
- If the context doesn't contain sufficient information, state this clearly
- Focus on practical, actionable information when possible
```

#### Step 5: Error Handling
Implement robust error handling for:
- Pinecone connection/query failures
- OpenAI API failures and rate limits  
- Invalid input validation
- Empty/insufficient search results
- Malformed document metadata

#### Step 6: Logging and Monitoring
Add structured logging for:
- Query processing time
- Number of results retrieved
- OpenAI token usage
- Error occurrences
- User query patterns (for analytics)

### 5. Code Structure

```python
# services/rag_service.py
class RAGService:
    def __init__(self, pinecone_client: Pinecone, openai_client: AsyncOpenAI):
        self.pinecone = pinecone_client
        self.openai = openai_client
    
    async def query_pinecone(self, question: str, max_results: int) -> List[Dict]:
        """Query Pinecone vector database with user question"""
        pass
    
    async def generate_response(self, question: str, context_docs: List[Dict], temperature: float) -> str:
        """Generate AI response using OpenAI with retrieved context"""
        pass
    
    def format_sources(self, pinecone_results: List[Dict]) -> List[SourceReference]:
        """Convert Pinecone results to structured source references"""
        pass
    
    async def process_rag_query(self, request: RAGQueryRequest) -> RAGResponse:
        """Main RAG processing pipeline"""
        pass

# routers/rag.py
@app.post("/api/rag/query", response_model=RAGResponse)
async def rag_query(
    request: RAGQueryRequest,
    rag_service: RAGService = Depends(get_rag_service)
):
    """RAG endpoint for question answering with source attribution"""
    pass
```

### 6. Expected Document Metadata Structure
Based on the provided example, expect these fields in Pinecone metadata:
- `document_id`: Unique identifier
- `title`: Document title
- `source_url`: Original URL
- `chunk`: Chunk identifier (e.g., "#8.0")
- `audience`: Target audience
- `publisher`: Publisher name
- `publication_year`: Year published
- `authority_level`: Authority classification
- `content`: The actual text content
- `word_range`: Word position range
- `symptoms`: Related symptoms/keywords

### 7. Testing Requirements
Create tests for:
- Valid query processing end-to-end
- Invalid input handling
- Pinecone connection failures
- OpenAI API failures
- Empty search results
- Malformed document metadata
- Response format validation

### 8. Performance Considerations
- Implement connection pooling for external APIs
- Add caching for frequently asked questions
- Consider async processing for better concurrency
- Monitor and limit OpenAI token usage
- Add request rate limiting

### 9. Security Notes
- Validate and sanitize all user inputs
- Implement proper API key management
- Add request size limits
- Consider implementing user authentication/authorization
- Log security-relevant events

### 10. Deployment Checklist
- Environment variable configuration
- Health check endpoints
- Monitoring and alerting setup
- API documentation with OpenAPI/Swagger
- Load testing with realistic query volumes

## Success Criteria
- Endpoint successfully queries Pinecone and returns relevant documents
- OpenAI generates coherent, well-cited responses
- JSON response follows the specified schema
- Proper error handling for all failure modes
- Response times under 10 seconds for typical queries
- Source references are accurate and properly formatted

## Example Usage
```bash
curl -X POST "http://localhost:8000/api/rag/query" \
     -H "Content-Type: application/json" \
     -d '{
       "question": "What are the symptoms of inflammatory bowel disease in dogs?",
       "max_results": 5,
       "temperature": 0.7
     }'
```

Expected response structure should include the AI answer with numbered citations and a sources array with full metadata for each referenced document.